dataset:
  data_dir: '/media/e210/HD81/Adhvaith_datasets/nsclc_public'
  data_pkl: '/media/e210/HD81/Adhvaith_datasets/results_patch/datasets/patches_dataset.pkl'

  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  input_size: [25, 25, 25]  # 3D input size (D, H, W)
  input_size_2d: 256  # 2D input size (square images)
  patch_size: [64, 64]  # Size of patches for 2D data
  patch_size_3d: [50, 50, 50]  # Size of patches for 3D data
  use_single_file: false  # Set to true to train on a single .npy file
  data_file: null  # Path to single .npy file if use_single_file is true
  test_mode: false  # Can be set at top level or here for backward compatibility
  n_test_samples: 20  # Can be set at top level or here for backward compatibility

# Settings for initial data preparation (before training)
preprocessing:  # Keep this as 'preprocessing' for compatibility
  normalize: true
  normalize_method: "95percentile"  # "95percentile", "minmax", "zscore", or "prescription"
  percentile_norm: 95  # Percentile for normalization (0-100)
  voxel_spacing: [1, 1, 1]  # Physical spacing in mm (isotropic or anisotropic)
  resize_to: [64, 64, 64]  # Target size for resampling, null to keep original size
  unit_type: "voxels"  # "voxels" or "mm" - how size values are interpreted
  extract_slices: false  # Whether to extract 2D slices from 3D volumes
  slice_extraction: "informative"  # "all", "center", or "informative"
  non_zero_threshold: 0.05  # For informative slice extraction
  num_workers: 8  # Number of parallel workers
  batch_size: 1  # Batch size for saving
  use_tanh_output: true  # Keep this here for compatibility

# Settings for transforms during training (conceptually separate but not used by preprocessing script)
transforms:
  augment: true  # Whether to apply data augmentation during training
  rotate: true  # Random rotation
  flip: true  # Random flipping
  adjust_brightness: true  # Random brightness adjustment
  random_crop: false  # Random cropping during training
  random_noise: false  # Add random noise during training

patch_extraction:
  enable: true  # Set to true to extract patches
  patch_dimension: "3D"  # "2D" or "3D" - dimensionality of patches to extract
  patch_size: [50, 50, 50]  # Size of patches to extract (in voxels or mm)
  patch_unit_type: "mm"  # "voxels" or "mm" - how patch_size values are interpreted
  threshold: 0.01  # Standard deviation threshold for informative patches
  max_patches_per_slice: 100  # Maximum number of patches to extract per slice
  max_patches_per_volume: 500  # Maximum number of patches to extract per volume
  random_state: 42  # Random seed for patch extraction

model:
  type: 'vae'  # Options: 'vae', 'resnet_ae', 'unet_ae', 'conv_autoencoder', 'mlp_autoencoder', 'unet'
  is_2d: false  # Set to true for 2D models
  latent_dim: 128
  in_channels: 1
  base_filters: 32
  bottleneck_filters: 256
  dropout_rate: 0.3  # Dropout rate for regularization

hyperparameters:
  learning_rate: 0.0001
  batch_size: 8
  epochs: 100
  early_stopping_patience: 10
  weight_decay: 0.0001
  beta: 1.0  # KL divergence weight for VAE

training:
  optimizer: 'adam'  # Options: 'adam', 'sgd', 'rmsprop'
  scheduler: 'reduce_on_plateau'  # Options: 'reduce_on_plateau', 'cosine_annealing', 'none'
  loss: 'mse'  # Options: 'mse', 'bce', 'combined'
  gpu_id: 0
  seed: 42
  grad_clip: 0.1  # Gradient clipping threshold
  non_zero_loss: false  # Whether to calculate loss only on non-zero regions
  accumulation_steps: 1  # Gradient accumulation steps

# Clinical metrics settings
clinical_metrics:
  use_pymedphys_gamma: true  # true to use PyMedPhys, false to use custom implementation
  gamma_dose_threshold: 3.0  # Dose difference criterion (%)
  gamma_distance_threshold: 3.0  # Distance-to-agreement (mm)
  dose_threshold_cutoff: 10  # Lower dose cutoff (% of max dose)
  calculate_dvh: true
  dvh_num_bins: 1000
  spacing: [2.0, 2.0, 2.0]  # Voxel spacing in mm

# Loss function configuration
loss_function:
  type: "combined"  # Options: "mse", "gamma", "dvh", "combined", "clinical"
  weights:
    mse: 1.0
    gamma: 0.1  # Weight for gamma pass rate loss (1 - pass_rate)
    dvh: 0.1    # Weight for DVH difference loss
    d95: 0.05   # Weight for D95 difference
    d50: 0.05   # Weight for D50 difference
    d2: 0.05    # Weight for D2 difference
  gamma_calculation_frequency: 10  # Calculate gamma every N batches (expensive operation)

# Logging configuration
logging:
  log_high_dose_slice: true
  log_frequency: 1  # Log high dose slice every N epochs
  log_reconstructions: true
  reconstruction_frequency: 10  # Log full reconstructions every N epochs
  num_samples_to_log: 4
  visualization_interval: 10  # Interval for logging image visualizations

wandb:
  use_wandb: true
  project_name: 'dose_distribution_autoencoder_patches'
  entity: 'pgsalome'
  name: 'doseae_patches'  # Run name (will be auto-generated if null)

optuna:
  use_optuna: false
  n_trials: 50
  timeout: 86400  # 24 hours
  direction: 'minimize'  # 'minimize' or 'maximize'
  sampler: 'tpe'  # Options: 'tpe', 'random', 'grid'
  pruner: 'median'  # Options: 'median', 'none'

output:
  model_dir: '/media/e210/HD81/Adhvaith_datasets/results_patch/models'
  log_dir: '/media/e210/HD81/Adhvaith_datasets/results_patch/logs'
  results_dir: '/media/e210/HD81/Adhvaith_datasets/results_patch'